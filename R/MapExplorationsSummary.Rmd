---
title: "MapExplorations"
output: html_document
---

## Goals
The idea is to explore how spatial aggregation changes the spatial pattern. Apparently, geographers call this the Modifiable Areal Unit Problem, so it's not a new idea. But, there is no agreed-upon solution to the MAUP other than perhaps not using spatial aggregation at all. 

To start, I'm just going to work on the simplest case, where we have spatial point data and we want to see it spatially aggregated. By starting with points, it will be easy to do the aggregation into differently-sized spatial bins, without having to do any up-, down-, or side-scaling. 

```{r, echo=FALSE, warning=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE)
```


```{r}
library(rCharts)
library(ggplot2)
library(foreign)
library(RgoogleMaps)
library(ggmap)
library(rgdal)
require(maptools)
require(dplyr)
```

### Stamen map 
I was able to get two Stamen maps of LA loaded, one interactive (not shown) and one static

```{r}
# map <- Leaflet$new()
# map$setView(c(34.0500, -118.2500), zoom = 13)
# map$tileLayer(provider = 'Stamen.Toner')
# map$marker(
#   'restaurants.json'
# )
# map
```

```{r}
map1 <- get_map(c(-118.2500, 34.0500), zoom = 8, maptype = "toner", source = "stamen")
map1 <- ggmap(map1)
map1
```

### Census tracts

For my smallest spatial aggregation unit, I'm using Census tracts. Previously, I was using blocks, but they were way too small. Essentially, they are *blocks* like the blocks on a street.

The shapefile data is provided by the Census, so it's reasonably easy to load the shapefiles and plot them over the base map. 
```{r}
#This reads in the shapefile data and then just grabs the LA stuff.
#tracts <- readOGR("tl_2013_06_tract", "tl_2013_06_tract")
#LAtracts <- tracts[tracts$COUNTYFP == "037",]
#writeOGR(LAtracts, ".", "LAtracts", driver="ESRI Shapefile")
```

```{r}
# read in data
LAtracts <- readOGR(".", layer="LAtracts")

# assign data ids? Getting this from ggplot2 tutorial on github
LAtracts@data$id = rownames(LAtracts@data)
LAtracts.points = fortify(LAtracts, region="id")
LAtracts.df = inner_join(LAtracts.points, LAtracts@data, by="id")
LAtracts <- spTransform(LAtracts, CRS("+proj=longlat +datum=WGS84"))
```

```{r tractmap}
plainpolys <- map1 + geom_polygon(aes(x=long, y=lat, group=id), size=.2, color='green', data=LAtracts.df, alpha=0)
plainpolys
```

## Korean restaurants

The first data I tried was scraped from yelp, and was all Korean restaurants listed in Los Angeles. Getting the data was a little tricky, and I'll have to re-use it for something else, because this wasn't the right application for it. But still. 

```{r}
clean_restaurants <- read.csv("~/Dropbox/Projects/MapExplorations/clean_restaurants.csv")
noNA <- subset(x=clean_restaurants, subset = is.na(clean_restaurants$LatLon.1)==FALSE)
rownames(noNA) <- NULL
coordinates(noNA) <- c("LatLon.2","LatLon.1")
proj4string(noNA) <- CRS("+proj=longlat +datum=WGS84")
```

To be able to plot the number of restaurants in each spatial area, we have to do a spatial search to see how many points are in each polygon.

```{r}
trial <- over(y=LAtracts, x=noNA)
numtracts <- summarise(group_by(trial, TRACTCE), numrests=n())
```

Then, we join the data about how many restaurants there are in each area back toward the tracts shapefile. 

```{r}
LAPlt <- left_join(LAtracts.df,numtracts, all.x=TRUE)
```

Was having trouble getting it to plot, but it seems like my problem was getting the names to match. 

```{r}
filledpolys <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numrests), data=LAPlt[is.na(LAPlt$numrests)==FALSE,])
colorscheme <- scale_fill_continuous(high="#FF0000", low="#4c0000")
filledpolys + colorscheme
```

### Neighborhoods

For a spatial aggregation level above the tract size, I found data about the boundaries of LA neighborhoods. 
```{r}
LAnbh <- readOGR("LANeighborhoodCouncils", layer="NeighborhoodCouncils")
proj4string(LAnbh) <- CRS("+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=6378137 +b=6378137 +units=m +no_defs") #not sure this is exactly right
LAnbh <- spTransform(LAnbh, CRS("+proj=longlat +datum=WGS84"))
LAnbh@data$id = rownames(LAnbh@data)
LAnbh.points = fortify(LAnbh, region="id")
LAnbh.df = inner_join(LAnbh.points, LAnbh@data, by="id")
```

Similar to the tracts, we can then calculate the number of restaurants within each polygon and then plot the spatial pattern.

```{r}
inNBH <- over(y=LAnbh, x=noNA)
numNBH<- summarise(group_by(inNBH, id), numrests=n())
LAnbh2 <- left_join(LAnbh.df,numNBH, all.x=TRUE)
```

```{r}
neighborhoods <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numrests), size=.2, data=LAnbh2[is.na(LAnbh2$numrests)==FALSE,])
neighborhoods + colorscheme
```

### Zipcodes
Next we use zipcodes.

```{r}
zips <- readOGR("CAMS_ZIPCODE_PARCEL_SPECIFIC", "CAMS_ZIPCODE_PARCEL_SPECIFIC")
```

```{r}
zips <- spTransform(zips, CRS("+proj=longlat +datum=WGS84"))
zips@data$id = rownames(zips@data)
zips.points = fortify(zips, region="id")
zips.df = inner_join(zips.points, zips@data, by="id")
```

```{r}
inZip <- over(y=zips, x=noNA)
numZip<- summarise(group_by(inZip, id), numrests=n())
zips2 <- left_join(zips.df,numZip, all.x=TRUE)
```

```{r}
zipsmap <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numrests), size=.2, data=zips2[is.na(zips2$numrests)==FALSE,])
zipsmap + colorscheme
```

## Streetlights

I want to have data with more coverage than the Korean restaurants, so I'm going to use that streetlight data I got about LA a while back. It doesn't make sense to plot absolute numbers (didn't really make sense with the restaurants, either) so I need to calculate lights per person in the particular spatial area. This means I need some population data.

```{r, echo=TRUE}
streetlights <- read.csv("../Streetlights/STLIGHT.csv")
head(streetlights)
```
Soooo, the names are wrong on this data. Let's switch that.
```{r}
streetlights <- rename(streetlights, latitude=Longitude, longitude=Latitude)
```
```{r}
rownames(streetlights) <- NULL
coordinates(streetlights) <-~longitude+latitude
proj4string(streetlights) <- CRS("+proj=longlat +datum=WGS84")
```

```{r}
lightsZip <- over(y=zips, x=streetlights)
numLightsZip<- summarise(group_by(lightsZip, ZIPCODE), numlights=n())
```

The population data came from the Census again (like most of the shapefiles).
```{r}
census <- read.csv("aff_download/DEC_10_SF1_SF1DP1_with_ann.csv", header = TRUE, skip=1)
LAcensus <- census[which(census$Id2 %in% numLightsZip$ZIPCODE),]
LAcensus <- LAcensus[, 1:4]
LAcensus$Id2 <- factor(LAcensus$Id2)
```

```{r}
numLightsZip2 <- inner_join(LAcensus, numLightsZip, c("Id2"="ZIPCODE"))
numLightsZip2$per <- numLightsZip2$Number..SEX.AND.AGE...Total.population/numLightsZip2$numlights
lights2 <- left_join(zips.df,numLightsZip2, all.x=TRUE, by=c("ZIPCODE"="Id2"))
```
We can plot both the absolute number of streetlights,
```{r}
lightmap1 <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numlights), size=.2, data=lights2[is.na(lights2$numlights)==FALSE,])
lightmap1 + colorscheme
```

and the number of people to each streetlight. So, if the scale says 100 that means there are 100 people to a streetlight in that particular spatial area.

```{r}
lightmap2 <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=per), size=.2, data=lights2[is.na(lights2$per)==FALSE,])
lightmap2 + scale_fill_gradient(name="people per streetlight", trans="log", breaks=c(1,10,100,1000), high="#FF0000", low="#4c0000")
```

What did we learn? Well, streetlights are basically distributed according to population by zipcode. When I had the plotting reversed (streetlights per person) Universal City was really standing out, I assume because no one lives there. 


For this data source, I worked from the largest spatial area to the smallest, so I need to go back and try this with smaller areas. But it's seeming likely that this isn't the right data for this problem either.

## Takeaways

This task is pretty hard in R. Getting the shapefiles loaded, and then verifying they loaded properly, is a task in and of itself. Then, getting the data to aggregate properly is also pretty hard. It's clear that some more abstractions would make this easier, for example if you could visually see the polygons and points on top of one another, demo on one spatial area what you wanted it to do, and have the system repeat the action n times where n is the number of polygons. This type of view would also let you more easily see what was getting aggregated. 

## Earthquake data

```{r}
earthquakes <- read.table("2014earthquakes.catalog", sep="")
names(earthquakes) <- c("Date", "Time", "ET", "MAG", "M", "lat", "long", "depth", "Q", "EVID", "NPH", "NGRM")
```

```{r}
rownames(earthquakes) <- NULL
coordinates(earthquakes) <-~long+lat
proj4string(earthquakes) <- CRS("+proj=longlat +datum=WGS84")
```

```{r}
quakesZip <- over(y=zips, x=earthquakes)
numQuakesZip<- summarise(group_by(quakesZip, ZIPCODE), numquakes=n())
quakesZip2 <- left_join(zips.df,numQuakesZip, all.x=TRUE)
```

```{r}
quakemap <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numquakes), size=.2, data=quakesZip2[is.na(quakesZip2$numquakes)==FALSE,])
quakemap + colorscheme
```


## More generic polygons

Now that I have some semi-decent data, I want to move to more generic polygons. This is going to be funny, because the shapes are going to be simpler but they're going to be much harder to create. 

```{r}
makePolys <- function(lat, lon, howmany){
  # Find the perimeter of the data
  latrange <- range(lat)
  lonrange <- range(lon)
  
  #start the boxes
  latitudes <- latrange[1]
  longitudes <- lonrange[1]
  
  #make breakpoints
  for (i in 1:(howmany)){
    latitudes[i+1] <- latitudes[i] + (latrange[2]-latrange[1])/howmany
    longitudes[i+1] <- longitudes[i] + (lonrange[2]-lonrange[1])/howmany
  }

  #build the boxes
  polys <- list()
  a <- 1
  for (i in 1:howmany){
    for (j in 1:howmany){
      polytmp <- data.frame(lon=c(longitudes[i], longitudes[i], longitudes[i+1], longitudes[i+1], longitudes[i]), lat=c(latitudes[j], latitudes[j+1], latitudes[j+1], latitudes[j], latitudes[j]))
      polytmp <- Polygon(polytmp)
      polys[a] <- Polygons(list(polytmp), letters[a])
      a <- a+ 1 
    }
  }
  return(polys)  
}

```{r}
polygons <- makePolys(quakesZip2$lat, quakesZip2$long, 5)
polygons = SpatialPolygons(polygons)
proj4string(polygons) <- CRS("+proj=longlat +datum=WGS84")

map1 + geom_polygon(aes(x=long, y=lat, group=id), size=.2, color='green', data=polygons, alpha=0)

polydata <- data.frame(id=letters[1:(howmany*howmany)], index=c(1:howmany*howmany))
rownames(polydata) <- letters[1:(howmany*howmany)]

polygons2 <- SpatialPolygonsDataFrame(polygons, polydata)

polygons2@data$id = rownames(polygons2@data)
polygons2.points = fortify(polygons2, region="id")
polygons2.df = inner_join(polygons2.points, polygons2@data, by="id")

quakesBox <- over(y=polygons2, x=earthquakes)
numQuakesBox <- summarise(group_by(quakesBox, id), numquakes=n())
quakesBox2 <- left_join(polygons2.df,numQuakesBox, all.x=TRUE)

quakemap <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numquakes), size=.2, data=quakesBox2[is.na(quakesBox2$numquakes)==FALSE,])
quakemap + colorscheme
```
