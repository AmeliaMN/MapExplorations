{
    "contents" : "---\ntitle: \"MapExplorations\"\noutput: html_document\n---\n\n## Goals\nThe idea is to explore how spatial aggregation changes the spatial pattern. Apparently, geographers call this the Modifiable Areal Unit Problem, so it's not a new idea. But, there is no agreed-upon solution to the MAUP other than perhaps not using spatial aggregation at all. \n\nTo start, I'm just going to work on the simplest case, where we have spatial point data and we want to see it spatially aggregated. By starting with points, it will be easy to do the aggregation into differently-sized spatial bins, without having to do any up-, down-, or side-scaling. \n\n```{r, echo=FALSE, warning=FALSE}\nlibrary(knitr)\nopts_chunk$set(cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE)\n```\n\n\n```{r}\nlibrary(rCharts)\nlibrary(ggplot2)\nlibrary(foreign)\nlibrary(RgoogleMaps)\nlibrary(ggmap)\nlibrary(rgdal)\nrequire(maptools)\nrequire(dplyr)\n```\n\n### Stamen map \nI was able to get two Stamen maps of LA loaded, one interactive (not shown) and one static\n\n```{r}\n# map <- Leaflet$new()\n# map$setView(c(34.0500, -118.2500), zoom = 13)\n# map$tileLayer(provider = 'Stamen.Toner')\n# map$marker(\n#   'restaurants.json'\n# )\n# map\n```\n\n```{r}\nmap1 <- get_map(c(-118.2500, 34.0500), zoom = 8, maptype = \"toner\", source = \"stamen\")\nmap1 <- ggmap(map1)\nmap1\n```\n\n### Census tracts\n\nFor my smallest spatial aggregation unit, I'm using Census tracts. Previously, I was using blocks, but they were way too small. Essentially, they are *blocks* like the blocks on a street.\n\nThe shapefile data is provided by the Census, so it's reasonably easy to load the shapefiles and plot them over the base map. \n```{r}\n#This reads in the shapefile data and then just grabs the LA stuff.\n#tracts <- readOGR(\"tl_2013_06_tract\", \"tl_2013_06_tract\")\n#LAtracts <- tracts[tracts$COUNTYFP == \"037\",]\n#writeOGR(LAtracts, \".\", \"LAtracts\", driver=\"ESRI Shapefile\")\n```\n\n```{r}\n# read in data\nLAtracts <- readOGR(\".\", layer=\"LAtracts\")\n\n# assign data ids? Getting this from ggplot2 tutorial on github\nLAtracts@data$id = rownames(LAtracts@data)\nLAtracts.points = fortify(LAtracts, region=\"id\")\nLAtracts.df = inner_join(LAtracts.points, LAtracts@data, by=\"id\")\nLAtracts <- spTransform(LAtracts, CRS(\"+proj=longlat +datum=WGS84\"))\n```\n\n```{r tractmap}\nplainpolys <- map1 + geom_polygon(aes(x=long, y=lat, group=id), size=.2, color='green', data=LAtracts.df, alpha=0)\nplainpolys\n```\n\n## Korean restaurants\n\nThe first data I tried was scraped from yelp, and was all Korean restaurants listed in Los Angeles. Getting the data was a little tricky, and I'll have to re-use it for something else, because this wasn't the right application for it. But still. \n\n```{r}\nclean_restaurants <- read.csv(\"~/Dropbox/Projects/MapExplorations/clean_restaurants.csv\")\nnoNA <- subset(x=clean_restaurants, subset = is.na(clean_restaurants$LatLon.1)==FALSE)\nrownames(noNA) <- NULL\ncoordinates(noNA) <- c(\"LatLon.2\",\"LatLon.1\")\nproj4string(noNA) <- CRS(\"+proj=longlat +datum=WGS84\")\n```\n\nTo be able to plot the number of restaurants in each spatial area, we have to do a spatial search to see how many points are in each polygon.\n\n```{r}\ntrial <- over(y=LAtracts, x=noNA)\nnumtracts <- summarise(group_by(trial, TRACTCE), numrests=n())\n```\n\nThen, we join the data about how many restaurants there are in each area back toward the tracts shapefile. \n\n```{r}\nLAPlt <- left_join(LAtracts.df,numtracts, all.x=TRUE)\n```\n\nWas having trouble getting it to plot, but it seems like my problem was getting the names to match. \n\n```{r}\nfilledpolys <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numrests), data=LAPlt[is.na(LAPlt$numrests)==FALSE,])\ncolorscheme <- scale_fill_continuous(high=\"#FF0000\", low=\"#4c0000\")\nfilledpolys + colorscheme\n```\n\n### Neighborhoods\n\nFor a spatial aggregation level above the tract size, I found data about the boundaries of LA neighborhoods. \n```{r}\nLAnbh <- readOGR(\"LANeighborhoodCouncils\", layer=\"NeighborhoodCouncils\")\nproj4string(LAnbh) <- CRS(\"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +a=6378137 +b=6378137 +units=m +no_defs\") #not sure this is exactly right\nLAnbh <- spTransform(LAnbh, CRS(\"+proj=longlat +datum=WGS84\"))\nLAnbh@data$id = rownames(LAnbh@data)\nLAnbh.points = fortify(LAnbh, region=\"id\")\nLAnbh.df = inner_join(LAnbh.points, LAnbh@data, by=\"id\")\n```\n\nSimilar to the tracts, we can then calculate the number of restaurants within each polygon and then plot the spatial pattern.\n\n```{r}\ninNBH <- over(y=LAnbh, x=noNA)\nnumNBH<- summarise(group_by(inNBH, id), numrests=n())\nLAnbh2 <- left_join(LAnbh.df,numNBH, all.x=TRUE)\n```\n\n```{r}\nneighborhoods <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numrests), size=.2, data=LAnbh2[is.na(LAnbh2$numrests)==FALSE,])\nneighborhoods + colorscheme\n```\n\n### Zipcodes\nNext we use zipcodes.\n\n```{r}\nzips <- readOGR(\"CAMS_ZIPCODE_PARCEL_SPECIFIC\", \"CAMS_ZIPCODE_PARCEL_SPECIFIC\")\n```\n\n```{r}\nzips <- spTransform(zips, CRS(\"+proj=longlat +datum=WGS84\"))\nzips@data$id = rownames(zips@data)\nzips.points = fortify(zips, region=\"id\")\nzips.df = inner_join(zips.points, zips@data, by=\"id\")\n```\n\n```{r}\ninZip <- over(y=zips, x=noNA)\nnumZip<- summarise(group_by(inZip, id), numrests=n())\nzips2 <- left_join(zips.df,numZip, all.x=TRUE)\n```\n\n```{r}\nzipsmap <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numrests), size=.2, data=zips2[is.na(zips2$numrests)==FALSE,])\nzipsmap + colorscheme\n```\n\n## Streetlights\n\nI want to have data with more coverage than the Korean restaurants, so I'm going to use that streetlight data I got about LA a while back. It doesn't make sense to plot absolute numbers (didn't really make sense with the restaurants, either) so I need to calculate lights per person in the particular spatial area. This means I need some population data.\n\n```{r, echo=TRUE}\nstreetlights <- read.csv(\"../Streetlights/STLIGHT.csv\")\nhead(streetlights)\n```\nSoooo, the names are wrong on this data. Let's switch that.\n```{r}\nstreetlights <- rename(streetlights, latitude=Longitude, longitude=Latitude)\n```\n```{r}\nrownames(streetlights) <- NULL\ncoordinates(streetlights) <-~longitude+latitude\nproj4string(streetlights) <- CRS(\"+proj=longlat +datum=WGS84\")\n```\n\n```{r}\nlightsZip <- over(y=zips, x=streetlights)\nnumLightsZip<- summarise(group_by(lightsZip, ZIPCODE), numlights=n())\n```\n\nThe population data came from the Census again (like most of the shapefiles).\n```{r}\ncensus <- read.csv(\"aff_download/DEC_10_SF1_SF1DP1_with_ann.csv\", header = TRUE, skip=1)\nLAcensus <- census[which(census$Id2 %in% numLightsZip$ZIPCODE),]\nLAcensus <- LAcensus[, 1:4]\nLAcensus$Id2 <- factor(LAcensus$Id2)\n```\n\n```{r}\nnumLightsZip2 <- inner_join(LAcensus, numLightsZip, c(\"Id2\"=\"ZIPCODE\"))\nnumLightsZip2$per <- numLightsZip2$Number..SEX.AND.AGE...Total.population/numLightsZip2$numlights\nlights2 <- left_join(zips.df,numLightsZip2, all.x=TRUE, by=c(\"ZIPCODE\"=\"Id2\"))\n```\nWe can plot both the absolute number of streetlights,\n```{r}\nlightmap1 <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numlights), size=.2, data=lights2[is.na(lights2$numlights)==FALSE,])\nlightmap1 + colorscheme\n```\n\nand the number of people to each streetlight. So, if the scale says 100 that means there are 100 people to a streetlight in that particular spatial area.\n\n```{r}\nlightmap2 <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=per), size=.2, data=lights2[is.na(lights2$per)==FALSE,])\nlightmap2 + scale_fill_gradient(name=\"people per streetlight\", trans=\"log\", breaks=c(1,10,100,1000), high=\"#FF0000\", low=\"#4c0000\")\n```\n\nWhat did we learn? Well, streetlights are basically distributed according to population by zipcode. When I had the plotting reversed (streetlights per person) Universal City was really standing out, I assume because no one lives there. \n\n\nFor this data source, I worked from the largest spatial area to the smallest, so I need to go back and try this with smaller areas. But it's seeming likely that this isn't the right data for this problem either.\n\n## Takeaways\n\nThis task is pretty hard in R. Getting the shapefiles loaded, and then verifying they loaded properly, is a task in and of itself. Then, getting the data to aggregate properly is also pretty hard. It's clear that some more abstractions would make this easier, for example if you could visually see the polygons and points on top of one another, demo on one spatial area what you wanted it to do, and have the system repeat the action n times where n is the number of polygons. This type of view would also let you more easily see what was getting aggregated. \n\n## Earthquake data\n\n```{r}\nearthquakes <- read.table(\"2014earthquakes.catalog\", sep=\"\")\nnames(earthquakes) <- c(\"Date\", \"Time\", \"ET\", \"MAG\", \"M\", \"lat\", \"long\", \"depth\", \"Q\", \"EVID\", \"NPH\", \"NGRM\")\n```\n\n```{r}\nrownames(earthquakes) <- NULL\ncoordinates(earthquakes) <-~long+lat\nproj4string(earthquakes) <- CRS(\"+proj=longlat +datum=WGS84\")\n```\n\n```{r}\nquakesZip <- over(y=zips, x=earthquakes)\nnumQuakesZip<- summarise(group_by(quakesZip, ZIPCODE), numquakes=n())\nquakesZip2 <- left_join(zips.df,numQuakesZip, all.x=TRUE)\n```\n\n```{r}\nquakemap <- map1 + geom_polygon(aes(x=long, y=lat, group=id, fill=numquakes), size=.2, data=quakesZip2[is.na(quakesZip2$numquakes)==FALSE,])\nquakemap + colorscheme\n```\n\n\n## More generic polygons\n\nNow that I have some semi-decent data, I want to move to more generic polygons. This is going to be funny, because the shapes are going to be simpler but they're going to be much harder to create. \n\n```{r}\nlatitudes <- range(quakesZip2$lat)\nmidlat <- latitudes[1] + (latitudes[2]-latitudes[1])/2\nlongitudes <- range(quakesZip2$long)\nmidlon <- longitudes[1]+ (longitudes[2]-longitudes[1])/2\n\npoly1 <- data.frame(lon=c(longitudes[1], longitudes[1], midlon, midlon, longitudes[1]), lat=c(latitudes[1], midlat, midlat, latitudes[1], latitudes[1]))\np <- Polygon(poly1)\nps = Polygons(list(p), 1)\n\npoly2 <- data.frame(lon=c(midlon, longitudes[2], longitudes[2], midlon, midlon), lat=c(latitudes[1], latitudes[1], midlat, midlat, latitudes[1]))\np <- Polygon(poly2)\nps2 <- Polygons(list(p), 2)\n\npoly3 <- data.frame(lon=c(midlon, longitudes[2], longitudes[2], midlon, midlon), lat=c(midlat, midlat, latitudes[2], latitudes[2], midlat))\np <- Polygon(poly3)\nps3 <- Polygons(list(p), 3)\n\npoly4 <- data.frame(lon=c(longitudes[1], midlon, midlon, longitudes[1], longitudes[1]), lat=c(midlat, midlat, latitudes[2], latitudes[2], midlat))\np <- Polygon(poly4)\nps4 <- Polygons(list(p), 4)\n\nsps =SpatialPolygons(list(ps, ps2, ps3, ps4))\nproj4string(sps) <- CRS(\"+proj=longlat +datum=WGS84\")\nmap1 + geom_polygon(aes(x=long, y=lat, group=id), size=.2, color='green', data=sps, alpha=0)\n```\n\nOkay, so those polygons are too big. Need to rethink this. ",
    "created" : 1420843913386.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3627228041",
    "id" : "87266A93",
    "lastKnownWriteTime" : 1421994961,
    "path" : "~/Dropbox/Projects/MapExplorations/MapExplorationsSummary.Rmd",
    "project_path" : "MapExplorationsSummary.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 0,
    "source_on_save" : false,
    "type" : "r_markdown"
}